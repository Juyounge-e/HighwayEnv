#!/usr/bin/env python3
"""
PPOë¥¼ ì‚¬ìš©í•œ Mixed Road í™˜ê²½ í›ˆë ¨ (M1 Mac ìµœì í™”)
==============================================

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” Stable-Baselines3ì˜ PPO ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ì—¬
custom-mixed-road í™˜ê²½ì—ì„œ ì—ì´ì „íŠ¸ë¥¼ í›ˆë ¨í•©ë‹ˆë‹¤.

M1 Mac ìµœì í™” íŠ¹ì§•:
- ë‹¨ì¼ í™˜ê²½ í›ˆë ¨ (ë³‘ë ¬ ì²˜ë¦¬ ì œê±°)
- MPS(Metal Performance Shaders) GPU ì§€ì›
- ê°„ì†Œí™”ëœ ì„¤ì •ìœ¼ë¡œ ë¹ ë¥¸ ì‹œì‘
- ì‹¤ì‹œê°„ í›ˆë ¨ ëª¨ë‹ˆí„°ë§
"""

import gymnasium as gym
import highway_env
import numpy as np
import matplotlib.pyplot as plt
from stable_baselines3 import PPO
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.callbacks import EvalCallback, BaseCallback
import torch
import os
import time
from typing import Dict, Any

class TrainingConfig:
    """M1 Macìš© ê°„ì†Œí™”ëœ í›ˆë ¨ ì„¤ì •"""
    
    def __init__(self):
        # í™˜ê²½ ì„¤ì •
        self.env_id = "custom-mixed-road-v0"
        self.max_episode_steps = 1000
        
        # PPO í•˜ì´í¼íŒŒë¼ë¯¸í„° (M1 Mac ìµœì í™”)
        self.total_timesteps = 100_000  # ë¡œì»¬ í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ì¤„ì„
        self.learning_rate = 3e-4
        self.n_steps = 1024  # ë‹¨ì¼ í™˜ê²½ìš©ìœ¼ë¡œ ì¤„ì„
        self.batch_size = 64
        self.n_epochs = 4    # ë¹ ë¥¸ í›ˆë ¨ì„ ìœ„í•´ ì¤„ì„
        self.gamma = 0.99
        self.gae_lambda = 0.95
        self.clip_range = 0.2
        self.ent_coef = 0.01
        self.vf_coef = 0.5
        self.max_grad_norm = 0.5
        
        # í™˜ê²½ë³„ ì„¤ì •
        self.env_config = {
            "observation": {"type": "Kinematics"},
            "action": {"type": "DiscreteMetaAction"},
            "vehicles_count": 3,   # ì°¨ëŸ‰ ìˆ˜ ëŒ€í­ ê°ì†Œ (10 -> 3)
            "controlled_vehicles": 1,
            "duration": 100,       # ì—í”¼ì†Œë“œ ê¸¸ì´ ì¦ê°€ (60 -> 100)
            "simulation_frequency": 15,
            "policy_frequency": 5,
            "normalize_reward": False,  # ì •ê·œí™” ë¹„í™œì„±í™”ë¡œ ë³´ìƒ í™•ì¸
            "collision_reward": -50,    # ì¶©ëŒ íŒ¨ë„í‹° ì¦ê°€
            "offroad_terminal": False,  # ë„ë¡œ ì´íƒˆì‹œ ì¦‰ì‹œ ì¢…ë£Œ ë°©ì§€
            "roundabout_exit_target": "north",
            "success_reward": 100.0,    # ì„±ê³µ ë³´ìƒ ì¦ê°€
            "completion_distance": 30,
        }
        
        # ë¡œê¹… ë° ì €ì¥
        self.log_dir = "./logs/"
        self.model_dir = "./models/"
        self.tensorboard_log = "./tensorboard/"
        self.eval_freq = 5000   # í‰ê°€ ì£¼ê¸°
        self.save_freq = 10000  # ëª¨ë¸ ì €ì¥ ì£¼ê¸°
        
        # M1 Mac GPU ì§€ì›
        self.device = self._get_device()
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(self.log_dir, exist_ok=True)
        os.makedirs(self.model_dir, exist_ok=True)
        os.makedirs(self.tensorboard_log, exist_ok=True)
    
    def _get_device(self):
        """M1 Macì— ìµœì í™”ëœ ë””ë°”ì´ìŠ¤ ì„ íƒ"""
        if torch.backends.mps.is_available():
            print("ğŸš€ MPS (Metal Performance Shaders) ì‚¬ìš© ê°€ëŠ¥!")
            return "mps"
        elif torch.cuda.is_available():
            print("ğŸ® CUDA GPU ì‚¬ìš© ê°€ëŠ¥!")
            return "cuda"
        else:
            print("ğŸ’» CPU ëª¨ë“œë¡œ ì‹¤í–‰")
            return "cpu"

class SimpleProgressCallback(BaseCallback):
    """ê°„ë‹¨í•œ ì§„í–‰ ìƒí™© ì½œë°±"""
    
    def __init__(self, check_freq: int = 1000):
        super().__init__()
        self.check_freq = check_freq
        self.start_time = None
        
    def _on_training_start(self) -> None:
        self.start_time = time.time()
        print("ğŸ¯ í›ˆë ¨ ì‹œì‘!")
        
    def _on_step(self) -> bool:
        if self.n_calls % self.check_freq == 0:
            elapsed_time = time.time() - self.start_time
            progress = self.n_calls / self.locals.get('total_timesteps', 1)
            
            print(f"ğŸ“Š ì§„í–‰ë¥ : {progress:.1%} ({self.n_calls:,} ìŠ¤í…)")
            print(f"   ì‹œê°„: {elapsed_time/60:.1f}ë¶„")
            
            # ìµœê·¼ ì—í”¼ì†Œë“œ ì •ë³´
            if 'infos' in self.locals and self.locals['infos']:
                info = self.locals['infos'][0]
                if 'episode' in info:
                    ep_info = info['episode']
                    print(f"   ìµœê·¼ ì—í”¼ì†Œë“œ: ë³´ìƒ={ep_info['r']:.2f}, ê¸¸ì´={ep_info['l']}")
        
        return True

def create_single_env(config: TrainingConfig):
    """ë‹¨ì¼ í™˜ê²½ ìƒì„±"""
    env = gym.make(config.env_id)
    env.unwrapped.configure(config.env_config)
    env = Monitor(env, config.log_dir + "training_env")
    return env

def evaluate_model_simple(model, env, n_episodes: int = 5):
    """ê°„ë‹¨í•œ ëª¨ë¸ í‰ê°€"""
    print(f"\nğŸ§ª ëª¨ë¸ í‰ê°€ ì¤‘... ({n_episodes}ê°œ ì—í”¼ì†Œë“œ)")
    
    episode_rewards = []
    success_count = 0
    
    for episode in range(n_episodes):
        obs, _ = env.reset()
        episode_reward = 0
        done = False
        
        while not done:
            action, _ = model.predict(obs, deterministic=True)
            obs, reward, terminated, truncated, info = env.step(action)
            episode_reward += reward
            done = terminated or truncated
        
        episode_rewards.append(episode_reward)
        
        # ì„±ê³µ ì—¬ë¶€ í™•ì¸
        if episode_reward > 30:
            success_count += 1
        
        print(f"   Episode {episode+1}: ë³´ìƒ={episode_reward:.2f}")
    
    avg_reward = np.mean(episode_rewards)
    success_rate = success_count / n_episodes
    
    print(f"ğŸ“Š í‰ê°€ ê²°ê³¼: í‰ê·  ë³´ìƒ={avg_reward:.2f}, ì„±ê³µë¥ ={success_rate:.1%}")
    
    return avg_reward, success_rate

def plot_simple_progress(log_dir: str, save_path: str):
    """ê°„ë‹¨í•œ í›ˆë ¨ ì§„í–‰ ìƒí™© ì‹œê°í™”"""
    try:
        import pandas as pd
        
        # ë¡œê·¸ íŒŒì¼ ì°¾ê¸°
        log_files = [f for f in os.listdir(log_dir) if f.endswith('.monitor.csv')]
        if not log_files:
            print("âŒ ë¡œê·¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # ë°ì´í„° ì½ê¸°
        df = pd.read_csv(os.path.join(log_dir, log_files[0]), skiprows=1)
        
        # ì‹œê°í™”
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
        fig.suptitle('PPO í›ˆë ¨ ì§„í–‰ ìƒí™©', fontsize=14)
        
        # ì—í”¼ì†Œë“œ ë³´ìƒ
        ax1.plot(df['r'], alpha=0.7)
        if len(df) > 10:
            # ì´ë™í‰ê· 
            window = min(20, len(df) // 5)
            moving_avg = df['r'].rolling(window=window).mean()
            ax1.plot(moving_avg, color='red', linewidth=2, label=f'ì´ë™í‰ê· ({window})')
            ax1.legend()
        
        ax1.set_title('ì—í”¼ì†Œë“œ ë³´ìƒ')
        ax1.set_xlabel('ì—í”¼ì†Œë“œ')
        ax1.set_ylabel('ë³´ìƒ')
        ax1.grid(True, alpha=0.3)
        
        # ì—í”¼ì†Œë“œ ê¸¸ì´
        ax2.plot(df['l'], alpha=0.7, color='green')
        ax2.set_title('ì—í”¼ì†Œë“œ ê¸¸ì´')
        ax2.set_xlabel('ì—í”¼ì†Œë“œ')
        ax2.set_ylabel('ìŠ¤í… ìˆ˜')
        ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        print(f"ğŸ“Š í›ˆë ¨ ì§„í–‰ ìƒí™© ì €ì¥: {save_path}")
        plt.close()
        
    except Exception as e:
        print(f"âŒ ì‹œê°í™” ì‹¤íŒ¨: {e}")

def main():
    """M1 Macìš© ê°„ì†Œí™”ëœ ë©”ì¸ í›ˆë ¨ í•¨ìˆ˜"""
    print("ğŸš— Mixed Road PPO í›ˆë ¨ (M1 Mac ìµœì í™”)")
    print("=" * 50)
    
    # ì„¤ì • ë¡œë“œ
    config = TrainingConfig()
    
    print(f"ğŸ”§ í›ˆë ¨ ì„¤ì •:")
    print(f"   â€¢ ì´ í›ˆë ¨ ìŠ¤í…: {config.total_timesteps:,}")
    print(f"   â€¢ ë””ë°”ì´ìŠ¤: {config.device}")
    print(f"   â€¢ ì°¨ëŸ‰ ìˆ˜: {config.env_config['vehicles_count']}")
    print(f"   â€¢ ì—í”¼ì†Œë“œ ê¸¸ì´: {config.env_config['duration']}")
    
    try:
        # í™˜ê²½ ìƒì„±
        print("\nğŸ”§ í™˜ê²½ ì„¤ì • ì¤‘...")
        train_env = create_single_env(config)
        eval_env = create_single_env(config)
        print("âœ… í™˜ê²½ ìƒì„± ì™„ë£Œ")
        
        # PPO ëª¨ë¸ ìƒì„±
        print("\nğŸ§  PPO ëª¨ë¸ ìƒì„± ì¤‘...")
        
        model = PPO(
            "MlpPolicy",
            train_env,
            learning_rate=config.learning_rate,
            n_steps=config.n_steps,
            batch_size=config.batch_size,
            n_epochs=config.n_epochs,
            gamma=config.gamma,
            gae_lambda=config.gae_lambda,
            clip_range=config.clip_range,
            ent_coef=config.ent_coef,
            vf_coef=config.vf_coef,
            max_grad_norm=config.max_grad_norm,
            tensorboard_log=config.tensorboard_log,
            device=config.device,
            verbose=1
        )
        
        print(f"âœ… PPO ëª¨ë¸ ìƒì„± ì™„ë£Œ")
        
        # ì½œë°± ì„¤ì •
        progress_callback = SimpleProgressCallback(check_freq=1000)
        
        eval_callback = EvalCallback(
            eval_env,
            best_model_save_path=config.model_dir,
            log_path=config.log_dir,
            eval_freq=config.eval_freq,
            deterministic=True,
            render=False,
            verbose=1,
            n_eval_episodes=3  # ë¹ ë¥¸ í‰ê°€ë¥¼ ìœ„í•´ ì¤„ì„
        )
        
        # í›ˆë ¨ ì‹œì‘
        print(f"\nğŸ¯ í›ˆë ¨ ì‹œì‘!")
        print("   TensorBoard ë¡œê·¸ë¥¼ ë³´ë ¤ë©´: tensorboard --logdir=./tensorboard/")
        print("   Ctrl+Cë¡œ ì–¸ì œë“  ì¤‘ë‹¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        
        start_time = time.time()
        
        model.learn(
            total_timesteps=config.total_timesteps,
            callback=[progress_callback, eval_callback],
            tb_log_name="PPO_MixedRoad_M1"
        )
        
        training_time = time.time() - start_time
        print(f"\nğŸ í›ˆë ¨ ì™„ë£Œ! (ì†Œìš” ì‹œê°„: {training_time/60:.1f}ë¶„)")
        
        # ìµœì¢… ëª¨ë¸ ì €ì¥
        final_model_path = os.path.join(config.model_dir, "ppo_mixed_road_m1.zip")
        model.save(final_model_path)
        print(f"ğŸ’¾ ìµœì¢… ëª¨ë¸ ì €ì¥: {final_model_path}")
        
        # ìµœì¢… í‰ê°€
        print("\nğŸ† ìµœì¢… ëª¨ë¸ í‰ê°€")
        final_avg_reward, final_success_rate = evaluate_model_simple(
            model, eval_env, n_episodes=10
        )
        
        # ë² ìŠ¤íŠ¸ ëª¨ë¸ í‰ê°€ (ìˆëŠ” ê²½ìš°)
        best_model_path = os.path.join(config.model_dir, "best_model.zip")
        if os.path.exists(best_model_path):
            print("\nğŸ¥‡ ë² ìŠ¤íŠ¸ ëª¨ë¸ í‰ê°€")
            best_model = PPO.load(best_model_path)
            best_avg_reward, best_success_rate = evaluate_model_simple(
                best_model, eval_env, n_episodes=10
            )
            
            print(f"\nğŸ“ˆ ê²°ê³¼ ë¹„êµ:")
            print(f"   ìµœì¢… ëª¨ë¸: ë³´ìƒ={final_avg_reward:.2f}, ì„±ê³µë¥ ={final_success_rate:.1%}")
            print(f"   ë² ìŠ¤íŠ¸ ëª¨ë¸: ë³´ìƒ={best_avg_reward:.2f}, ì„±ê³µë¥ ={best_success_rate:.1%}")
        
        # í›ˆë ¨ ì§„í–‰ ìƒí™© ì‹œê°í™”
        plot_path = os.path.join(config.model_dir, "training_progress.png")
        plot_simple_progress(config.log_dir, plot_path)
        
        print(f"\nğŸ‰ í›ˆë ¨ ì™„ë£Œ!")
        print(f"   â€¢ ëª¨ë¸ ì €ì¥ ìœ„ì¹˜: {config.model_dir}")
        print(f"   â€¢ ë¡œê·¸ ìœ„ì¹˜: {config.log_dir}")
        print(f"   â€¢ ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ í…ŒìŠ¤íŠ¸: python test_trained_model.py")
        
    except KeyboardInterrupt:
        print("\nâš ï¸  ì‚¬ìš©ìì— ì˜í•´ í›ˆë ¨ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print("   ë¶€ë¶„ì ìœ¼ë¡œ í›ˆë ¨ëœ ëª¨ë¸ì´ ì €ì¥ë˜ì—ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        
    except Exception as e:
        print(f"\nâŒ í›ˆë ¨ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        
    finally:
        # í™˜ê²½ ì •ë¦¬
        try:
            train_env.close()
            eval_env.close()
        except:
            pass

if __name__ == "__main__":
    main() 